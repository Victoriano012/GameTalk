

llms:
  train_llm_name: "llama-3B"
  opponent_llm_name: ${llms.train_llm_name}
  unsloth : False

game:
  name: "rock-paper-scissors"
  stopping_text: ["</talk>", "</play>"]
  one_turn : False

  player_1_name: "Player-1"
  player_2_name: "Player-2"
  trained_player: 2   # 1 or 2 or "both"

train:
  lr: 1e-5
  epochs: 500
  batches_per_epoch: 1
  group_size: 16
  minibatch_size: 1
  gradient_accumulation: True
  
  max_interactions: 5
  max_new_tokens: 200

  gamma : 1
  kl_coef: 0.1
  ppo_eps: 0.2
  grpo_std_eps: 1e-8

  save_every: 50

  const_ref_llm : True

  replay_batches_per_epoch: 3
  buffer_size_limit: 16

  opponent_temperature : 0.6
  trained_temperature : 1
  opponent_top_p : 0.9
  trained_top_p : 0.5
  
  method : "grpo" # "dpo" / "grpo"
  dpo:
    variant : "all_pairs" # "all_pairs" / "all_perms" / "with_ties"
    average_log_prob : False


eval:
  eval_every: 10
  num_episodes: 16
  
  opponent_temperature : ${train.opponent_temperature}
  trained_temperature : ${train.trained_temperature}
  opponent_top_p : ${train.opponent_top_p}
  trained_top_p : ${train.trained_top_p}


lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.0

prompts:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/prompts/RPS/"
  initial: "initial.txt"
  other_moved: "other_moved.txt"

logs:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/logs/RPS/"
  general: "general.txt"
  conversations: "conversations.txt"
  metrics: "metrics.txt"
  model: "model.pth"

run_name: "first-test"
wandb:
  project: "rock-paper-scissors"



###### Not modifiable configuration ######

tracked_metrics: [
  "win_rate",
  "draw_rate",
  "loss_rate",
  "reward",
  "opponent_reward",
  "normalized_relative_advantage",
  "|kl|",
  "|total_loss|",
  "replay_|kl|",
  "replay_|total_loss|",
  # "internal_state_loss",   # only tracked when evaluating
  # "word_based_loss",       # only tracked when evaluating

  "num_samples",
  "num_interactions",
  "conversation_length (tokens)", # just root conversation
  # "num_error_conversations",
  "lost_by_error (%)",
  "won_by_error (%)",
  # "mini-errors",

  "memory_usage_generation (GB)",
  "memory_usage_minibatch (GB)",
  "time_generation (s)",
  "time_minibatch (s)",
  "time_total_minibatches (s)",
  "replay_time_minibatch (s)",
  "time_total_replay (s)"
]

# metrics to be divided by number of samples
normalized_metrics: [
  "reward",
  "opponent_reward",
  "win_rate",
  "draw_rate",
  "loss_rate",
  "lost_by_error (%)",
  "won_by_error (%)",
  "num_interactions"
]

bad_words: [
  "kill"
]