

llms:
  train_llm_name: "llama-3B"
  opponent_llm_name: ${llms.train_llm_name}
  unsloth : False # not working

train:
  resume_from_checkpoint : False

  ####### Trainer Config #######
  learning_rate: 1e-4
  num_train_epochs: 500
  auto_find_batch_size : True
  gradient_accumulation_steps : 1

  eval_strategy: "steps"
  eval_steps: 20
  eval_on_start : False
  logging_steps : 1
  logging_first_step: True
  save_steps : ${train.eval_steps}
  save_total_limit : 2
  load_best_model_at_end : True
  metric_for_best_model : "reward"
  early_stopping_patience: 100
  
  num_generations: 8
  temperature: ${train.trained_temperature}
  
  beta: 0.1 # kl_coef

  leverageOpportunity_weight : 1.

  ####### GRPOTrainer Config ##########
  epsilon : 0.2 # ppo_epsilon
  num_iterations : 1 # GRPO mu
  max_completion_length : 300
  max_prompt_length : null
  entropy_beta: 0.0

  ####### OnlineDPOTrainer Config #####
  max_new_tokens : ${train.max_completion_length}
  max_length : 100000000000

  ####### STaRTrainer Config ##########
  min_sft_part : 0.1
  max_steps: 3000
  dataset_kwargs :
    skip_prepare_dataset : True
  star_batch_size : 1

  ####### Custom Config ###############
  eval_samples : 32

  #####################################

  trained_temperature : 1
  # trained_top_p : 1.0 (GRPOTrainer doesn't allow it)
  opponent_temperature : 0.6
  opponent_top_p : 0.9
  
  method : "grpo" # "dpo" / "grpo" / "star"
  dpo_variant : "all_pairs" # "all_pairs" / "all_perms" / "with_ties"

  gamma : 1 # TODO (should rewrite compute_loss, not trivial)


dataset:
  num_root_generations : 4
  samples_per_epoch : 8 # ~ dataset_size
  max_interactions: 5

  game_name: "rock-paper-scissors"

  player_1_name: "Player-1"
  player_2_name: "Player-2"
  trained_player: 2  # 1 or 2 or "both"
  player_A_num: 2    # 1 or 2 or "both" (for SPbargaining)

  data : "/home/azureuser/main/LLMGameTheory/experiments/data/RPS.pkl"


lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.0

prompts:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/prompts/RPS/"
  initial_A: "initial.txt"
  initial_B: "initial.txt"
  intermediate: "other_moved.txt"

logs:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/logs/RPS/iteration_3/"
  general: "general.txt"
  metrics: "metrics.txt"
  conversations: "conversations.txt"
  eval_conversations: "eval_conversations.txt"

run_name: "first-test"
wandb:
  project: ${dataset.game_name}
  tags :
    - ${train.method}
    - ${dataset.game_name}
    - ${llms.train_llm_name}


###### Not modifiable configuration ######
bad_words: [
  "kill"
]