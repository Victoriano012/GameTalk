
game_name: "rock-paper-scissors"

player_1_name: "Player-1"
player_2_name: "Player-2"
train_llm_name: "meta-llama/Llama-3.2-3B-Instruct"
opponent_llm_name: "meta-llama/Llama-3.2-3B-Instruct"
trained_player: 2   # 1 or 2 or "both"

train:
  lr: 1e-5
  epochs: 500
  batches_per_epoch: 1
  group_size: 32
  minibatch_size: 4
  
  max_interactions: 5
  max_new_tokens: 200

  kl_coef: 0.1
  ppo_eps: 0.2
  grpo_std_eps: 1e-8

  save_every: 50

  replay_batches_per_epoch: 3
  buffer_size_limit: 16


eval:
  eval_every: 10
  num_episodes: 16


lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.0

prompts:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/prompts/RPS/"
  initial: "initial.txt"
  other_moved: "other_moved.txt"

logs:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/logs/RPS/"
  general: "general.txt"
  conversations: "conversations.txt"
  metrics: "metrics.txt"
  model: "model.pth"

run_name: "first-test"
wandb:
  project: "rock-paper-scissors"



###### Not modifiable configuration ######

tracked_metrics: [
  "win_rate",
  "draw_rate",
  "loss_rate",
  "reward",
  "opponent_reward",
  "normalized_relative_advantage",
  "kl",
  "total_loss",
  "replay_kl",
  "replay_total_loss",
  "internal_state_loss",

  "num_samples",
  "num_interactions", # just root conversation
  "conversation_length (tokens)", # just root conversation
  "num_error_conversations",
  "lost_by_error",
  "won_by_error",

  "memory_usage_generation (GB)",
  "memory_usage_minibatch (GB)",
  "time_generation (s)",
  "time_minibatch (s)",
  "time_total_minibatches (s)",
  "replay_time_minibatch (s)",
  "time_total_replay (s)"
]

# metrics to be divided by number of samples
normalized_metrics: [
  "reward",
  "opponent_reward",
  "total_loss",
  "kl",
  "win_rate",
  "draw_rate",
  "loss_rate"
]

bad_words: [
  "kill"
]