

llms:
  train_llm_name: "llama-3B"
  opponent_llm_name: ${llms.train_llm_name}
  unsloth : False # not working

train:
  learning_rate: 1e-4
  num_train_epochs: 500
  auto_find_batch_size : True
  num_generations: 8
  gradient_accumulation_steps : 1
  early_stopping_patience: 100
  
  max_completion_length: 300

  gamma : 1 # TODO (should rewrite compute_loss, not trivial)

  beta: 0.1 # kl_coef
  epsilon : 0.2 # ppo_epsilon

  num_iterations : 1

  trained_temperature : 1
  # trained_top_p : 1.0 (GRPOTrainer doesn't allow it)
  opponent_temperature : 0.6
  opponent_top_p : 0.9
  
  # TODO
  method : "grpo" # "dpo" / "grpo"
  dpo:
    variant : "all_pairs" # "all_pairs" / "all_perms" / "with_ties"
    average_log_prob : False

  eval_steps: 50
  logging_steps : 1
  save_steps : ${train.eval_steps}
  save_total_limit : 2
  # load_best_model_at_end : True

dataset:
  num_root_generations : 4
  samples_per_epoch : 8 # ~ dataset_size
  max_interactions: 5

  game_name: "rock-paper-scissors"

  player_1_name: "Player-1"
  player_2_name: "Player-2"
  trained_player: 2   # 1 or 2 or "both"

# TODO
eval:
  num_samples: 16
  
  opponent_temperature : ${train.opponent_temperature}
  trained_temperature : ${train.trained_temperature}
  opponent_top_p : ${train.opponent_top_p}
  trained_top_p : ${train.trained_top_p}


lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.0

prompts:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/prompts/RPS/"
  initial: "initial.txt"
  other_moved: "other_moved.txt"

logs:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/logs/RPS/iteration_3/"
  general: "general.txt"
  conversations: "conversations.txt"
  metrics: "metrics.txt"

run_name: "first-test"
wandb:
  project: "rock-paper-scissors"


# TODO
###### Not modifiable configuration ######

tracked_metrics: [
  "win_rate",
  "draw_rate",
  "loss_rate",
  "reward",
  "opponent_reward",
  "normalized_relative_advantage",
  "|kl|",
  "|total_loss|",
  "replay_|kl|",
  "replay_|total_loss|",
  # "internal_state_loss",   # only tracked when evaluating
  # "word_based_loss",       # only tracked when evaluating

  "num_samples",
  "num_interactions",
  "conversation_length (tokens)", # just root conversation
  # "num_error_conversations",
  "lost_by_error (%)",
  "won_by_error (%)",
  # "mini-errors",

  "memory_usage_generation (GB)",
  "memory_usage_minibatch (GB)",
  "time_generation (s)",
  "time_minibatch (s)",
  "time_total_minibatches (s)",
  "replay_time_minibatch (s)",
  "time_total_replay (s)"
]

# metrics to be divided by number of samples
normalized_metrics: [
  "reward",
  "opponent_reward",
  "win_rate",
  "draw_rate",
  "loss_rate",
  "lost_by_error (%)",
  "won_by_error (%)",
  "num_interactions"
]

bad_words: [
  "kill"
]