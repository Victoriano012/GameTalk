

llms:
  train_llm_name: "llama-3B"
  opponent_llm_name: ${llms.train_llm_name}
  unsloth : False # not working

train:
  learning_rate: 1e-4
  num_train_epochs: 500
  auto_find_batch_size : True
  num_generations: 8
  gradient_accumulation_steps : 1
  early_stopping_patience: 10
  
  max_completion_length: 300

  gamma : 1 # TODO (should rewrite compute_loss, not trivial)

  beta: 0.1 # kl_coef
  epsilon : 0.2 # ppo_epsilon

  num_iterations : 1 # GRPO mu

  trained_temperature : 1
  # trained_top_p : 1.0 (GRPOTrainer doesn't allow it)
  opponent_temperature : 0.6
  opponent_top_p : 0.9

  eval_steps: 20
  eval_samples : 32
  eval_on_start : True
  logging_steps : 1
  save_steps : ${train.eval_steps}
  save_total_limit : 2
  load_best_model_at_end : True
  
  # TODO
  method : "grpo" # "dpo" / "grpo"
  dpo:
    variant : "all_pairs" # "all_pairs" / "all_perms" / "with_ties"
    average_log_prob : False

dataset:
  num_root_generations : 4
  samples_per_epoch : 8 # ~ dataset_size
  max_interactions: 5

  game_name: "rock-paper-scissors"

  player_1_name: "Player-1"
  player_2_name: "Player-2"
  trained_player: 2   # 1 or 2 or "both"


lora:
  r: 32
  lora_alpha: 64
  lora_dropout: 0.0

prompts:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/prompts/RPS/"
  initial: "initial.txt"
  other_moved: "other_moved.txt"

logs:
  folder: "/home/azureuser/main/LLMGameTheory/experiments/logs/RPS/iteration_3/"
  general: "general.txt"
  metrics: "metrics.txt"
  conversations: "conversations.txt"
  eval_conversations: "eval_conversations.txt"

run_name: "first-test"
wandb:
  project: ${dataset.game_name}
  tags : ["iteration_3"]


###### Not modifiable configuration ######
bad_words: [
  "kill"
]